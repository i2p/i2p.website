---
title: "Производительность"
description: "Производительность сети I2P: как она работает сегодня, исторические улучшения и идеи для будущей оптимизации"
slug: "performance"
lastUpdated: "2025-10"
accurateFor: "2.10.0"
reviewStatus: "needs-review"
---

## Производительность сети I2P: Скорость, соединения и управление ресурсами

Сеть I2P является полностью динамичной. Каждый клиент известен другим узлам и тестирует локально известные узлы на предмет доступности и производительности. Только доступные и производительные узлы сохраняются в локальную NetDB. В процессе построения туннелей из этого пула выбираются лучшие ресурсы для создания туннелей. Поскольку тестирование происходит непрерывно, пул узлов изменяется. Каждый узел I2P знает разные части NetDB, что означает, что каждый router имеет различный набор узлов I2P для использования в туннелях. Даже если два router'а имеют одинаковое подмножество известных узлов, тесты на доступность и производительность, вероятно, покажут разные результаты, поскольку другие router'ы могут находиться под нагрузкой именно в момент тестирования одним router'ом, но быть свободными, когда второй router проводит тестирование.

Это объясняет, почему каждый узел I2P использует разные узлы для построения туннелей. Поскольку каждый узел I2P имеет различную задержку и пропускную способность, туннели (которые строятся через эти узлы) имеют разные значения задержки и пропускной способности. И поскольку каждый узел I2P строит разные туннели, ни у каких двух узлов I2P нет одинаковых наборов туннелей.

Сервер/клиент известен как "destination", и каждый destination имеет по крайней мере один входящий и один исходящий tunnel. По умолчанию используется 3 перехода на tunnel. Это составляет в общей сложности 12 переходов (12 различных узлов I2P) для полного цикла клиент → сервер → клиент.

Каждый пакет данных проходит через 6 других узлов I2P, прежде чем достигнет сервера:

клиент - узел1 - узел2 - узел3 - узелA1 - узелA2 - узелA3 - сервер

и на обратном пути 6 различных I2P-узлов:

server - hopb1 - hopb2 - hopb3 - hopc1 - hopc2 - hopc3 - client

Трафику в сети требуется ACK перед отправкой новых данных; необходимо дождаться возвращения ACK от сервера: отправить данные, ждать ACK, отправить ещё данные, ждать ACK. Поскольку RTT (Round Trip Time, время прохождения сигнала туда и обратно) складывается из задержки каждого отдельного узла I2P и каждого соединения на этом пути туда-обратно, обычно проходит 1–3 секунды, пока ACK вернётся к клиенту. Из-за особенностей проектирования TCP и транспорта I2P пакет данных имеет ограниченный размер. Вместе эти условия устанавливают предел максимальной пропускной способности на туннель примерно в 20–50 кБ/с. Однако если хотя бы один узел (hop) в туннеле располагает только 5 кБ/с пропускной способности, весь туннель ограничивается 5 кБ/с, независимо от задержки и других ограничений.

Шифрование, задержка и процесс построения туннеля делают создание туннеля довольно затратным по процессорному времени. Поэтому destination может иметь максимум 6 входящих и 6 исходящих туннелей для передачи данных. При максимальной пропускной способности 50 кБ/с на туннель, destination может использовать примерно 300 кБ/с суммарного трафика (в реальности это может быть больше, если используются более короткие туннели с низкой или отсутствующей анонимностью). Используемые туннели уничтожаются каждые 10 минут, и строятся новые. Эта смена туннелей, а также иногда клиенты, которые завершают работу или теряют соединение с сетью, могут приводить к разрыву туннелей и соединений. Пример этого можно увидеть в IRC2P Network при потере соединения (ping timeout) или при использовании eepget.

При ограниченном наборе назначений и ограниченном количестве туннелей на каждое назначение, один узел I2P использует лишь ограниченный набор туннелей через другие узлы I2P. Например, если узел I2P является «hop1» в небольшом примере выше, он видит только один участвующий туннель, исходящий от клиента. Если суммировать всю сеть I2P, можно построить лишь относительно ограниченное число участвующих туннелей с ограниченным общим объёмом пропускной способности. Если распределить эти ограниченные числа по количеству узлов I2P, для использования доступна лишь малая часть имеющейся пропускной способности/ёмкости.

Чтобы сохранить анонимность, один router не должен использоваться всей сетью для построения туннелей. Если один router действительно выступает в роли tunnel router для всех узлов I2P, он становится реальной единой точкой отказа, а также центральной точкой для сбора IP-адресов и данных от клиентов. Вот почему сеть распределяет трафик между узлами в процессе построения туннелей.

Еще одним фактором, влияющим на производительность, является способ, которым I2P обрабатывает mesh-сети. Каждый переход между узлами (hop-to-hop) использует одно TCP или UDP соединение на узлах I2P. При 1000 соединений наблюдается 1000 TCP-соединений. Это довольно много, и некоторые домашние маршрутизаторы и маршрутизаторы малых офисов допускают лишь небольшое количество соединений. I2P пытается ограничить эти соединения до 1500 на каждый тип — UDP и TCP. Это также ограничивает объем трафика, маршрутизируемого через узел I2P.

Если узел доступен, имеет настройку пропускной способности >128 КБ/с для общего использования и доступен 24/7, он должен использоваться через некоторое время для участия в трафике. Если он в промежутке времени отключается, тестирование узла I2P, проводимое другими узлами, покажет им, что он недоступен. Это блокирует узел на других узлах как минимум на 24 часа. Таким образом, другие узлы, которые протестировали этот узел как недоступный, не будут использовать его для построения туннелей в течение 24 часов. Вот почему ваш трафик ниже после перезапуска/выключения вашего I2P router минимум в течение 24 часов.

Кроме того, другим узлам I2P необходимо знать I2P router для проверки его доступности и пропускной способности. Этот процесс можно ускорить, взаимодействуя с сетью, например, используя приложения или посещая I2P-сайты, что приведёт к большему количеству построений tunnel и, следовательно, к большей активности и доступности для тестирования узлами сети.

## История производительности (избранное)

За годы существования I2P было реализовано несколько значительных улучшений производительности:

### Native math

Реализовано через JNI-привязки к библиотеке GNU MP (GMP) для ускорения операции `modPow` в BigInteger, которая ранее занимала основную часть процессорного времени. Первые результаты показали значительное ускорение криптографии с открытым ключом. См.: /misc/jbigi/

### Garlic wrapping a "reply" LeaseSet (tuned)

Ранее для ответов часто требовался поиск LeaseSet отправителя в сетевой базе данных. Включение LeaseSet отправителя в начальный garlic улучшает задержку ответа. Теперь это делается выборочно (в начале соединения или при изменении LeaseSet) для снижения накладных расходов.

### Нативная математика

Перенесены некоторые шаги валидации на более ранний этап процесса установления соединения transport, чтобы отклонять некорректные узлы быстрее (неправильные часы, неправильная настройка NAT/firewall, несовместимые версии), экономя CPU и трафик.

### Упаковка LeaseSet для "ответа" в garlic (настроенная)

Используйте контекстно-зависимое тестирование tunnel: избегайте тестирования tunnel, которые уже передают данные; отдавайте предпочтение тестированию в режиме простоя. Это снижает накладные расходы и ускоряет обнаружение неисправных tunnel.

### Более эффективное отклонение TCP

Сохранение выбранных туннелей для данного соединения уменьшает доставку пакетов не по порядку и позволяет библиотеке потоковой передачи увеличивать размеры окон, улучшая пропускную способность.

### Корректировки тестирования туннелей

GZip или аналогичные методы для объёмных структур (например, опций RouterInfo) снижают использование пропускной способности там, где это целесообразно.

### Постоянный выбор tunnel/lease

Замена упрощённого протокола «ministreaming». Современный streaming включает выборочные ACK и управление перегрузкой, адаптированные под анонимный, ориентированный на сообщения субстрат I2P. См.: /docs/api/streaming/

## Future Performance Improvements (historical ideas)

Ниже представлены идеи, задокументированные исторически как потенциальные улучшения. Многие из них устарели, реализованы или заменены архитектурными изменениями.

### Сжимать выбранные структуры данных

Улучшить механизм выбора роутерами узлов для построения туннелей, чтобы избежать медленных или перегруженных узлов, сохраняя при этом устойчивость к Sybil-атакам со стороны мощных противников.

### Полный протокол потоковой передачи

Уменьшите ненужное исследование, когда пространство ключей стабильно; настройте, сколько узлов возвращается в запросах и сколько одновременных поисков выполняется.

### Session Tag tuning and improvements (legacy)

Для устаревшей схемы ElGamal/AES+SessionTag более умные стратегии истечения и пополнения снижают количество откатов к ElGamal и неиспользованных тегов.

### Улучшенное профилирование и выбор пиров

Генерировать теги из синхронизированного PRNG, инициализированного при установке новой сессии, что снижает накладные расходы на сообщение по сравнению с предварительно доставленными тегами.

### Настройка сетевой базы данных

Более длительное время жизни туннелей в сочетании с восстановлением может снизить накладные расходы на перестроение; необходимо найти баланс между анонимностью и надёжностью.

### Настройка и улучшения Session Tag (устаревшее)

Отклонять недействительные узлы раньше и делать тесты туннелей более контекстно-зависимыми для снижения конкуренции и задержки.

### Миграция SessionTag на синхронизированный PRNG (устаревший)

Выборочное объединение LeaseSet, опции сжатия RouterInfo и внедрение полного протокола потоковой передачи — всё это способствует улучшению воспринимаемой производительности.

Не вижу текста для перевода. Пожалуйста, предоставьте текст, который нужно перевести с английского на русский.

Смотрите также:

- [Маршрутизация туннелей](/docs/overview/tunnel-routing/)
- [Выбор пиров](/docs/overview/tunnel-routing/)
- [Транспорты](/docs/overview/transport/)
- [Спецификация SSU2](/docs/specs/ssu2/) и [Спецификация NTCP2](/docs/specs/ntcp2/)
