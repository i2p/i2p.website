---
title: "성능"
description: "I2P 네트워크 성능: 현재 동작 방식, 역사적 개선 사항, 그리고 향후 튜닝 아이디어"
slug: "performance"
lastUpdated: "2025-10"
accurateFor: "2.10.0"
reviewStatus: "needs-review"
---

## I2P 네트워크 성능: 속도, 연결 및 리소스 관리

I2P 네트워크는 완전히 동적입니다. 각 클라이언트는 다른 노드들에게 알려지며, 로컬에 알려진 노드들의 도달 가능성과 용량을 테스트합니다. 도달 가능하고 충분한 성능을 가진 노드만 로컬 NetDB에 저장됩니다. tunnel 구축 과정에서 이 풀에서 최적의 리소스가 선택되어 tunnel을 구축하는 데 사용됩니다. 테스트가 지속적으로 수행되기 때문에 노드 풀은 계속 변경됩니다. 각 I2P 노드는 NetDB의 서로 다른 부분을 알고 있으며, 이는 각 router가 tunnel에 사용할 수 있는 서로 다른 I2P 노드 집합을 가지고 있음을 의미합니다. 두 router가 동일한 알려진 노드 하위 집합을 가지고 있더라도, 도달 가능성과 용량 테스트는 서로 다른 결과를 보일 가능성이 높습니다. 한 router가 테스트할 때는 다른 router들이 부하 상태에 있을 수 있지만, 두 번째 router가 테스트할 때는 여유 상태일 수 있기 때문입니다.

이것은 각 I2P 노드가 터널을 구축하기 위해 서로 다른 노드를 사용하는 이유를 설명합니다. 모든 I2P 노드는 지연시간과 대역폭이 다르기 때문에, 해당 노드들을 통해 구축된 터널은 서로 다른 지연시간과 대역폭 값을 가집니다. 그리고 모든 I2P 노드가 서로 다른 터널을 구축하기 때문에, 동일한 터널 집합을 가진 두 I2P 노드는 존재하지 않습니다.

서버/클라이언트는 "destination"으로 알려져 있으며, 각 destination은 최소 하나의 인바운드 터널과 하나의 아웃바운드 터널을 가지고 있습니다. 기본값은 터널당 3홉입니다. 이는 전체 왕복 경로인 클라이언트 → 서버 → 클라이언트에 대해 총 12홉(12개의 서로 다른 I2P 노드)이 됩니다.

각 데이터 패키지는 서버에 도달하기 전에 6개의 다른 I2P 노드를 거쳐 전송됩니다:

client - hop1 - hop2 - hop3 - hopa1 - hopa2 - hopa3 - server

그리고 돌아오는 길에는 6개의 서로 다른 I2P 노드를 거칩니다:

server - hopb1 - hopb2 - hopb3 - hopc1 - hopc2 - hopc3 - client

네트워크 트래픽은 새로운 데이터를 전송하기 전에 ACK를 필요로 합니다. 서버로부터 ACK가 돌아올 때까지 기다려야 합니다: 데이터 전송, ACK 대기, 추가 데이터 전송, ACK 대기. RTT(Round Trip Time, 왕복 시간)는 이 왕복 경로의 각 I2P 노드와 각 연결의 지연 시간이 누적되므로, 일반적으로 ACK가 클라이언트로 돌아오는 데 1–3초가 소요됩니다. TCP와 I2P 전송 설계로 인해 데이터 패키지는 제한된 크기를 가집니다. 이러한 조건들이 결합되어 tunnel당 최대 대역폭은 대략 20–50 kB/s로 제한됩니다. 그러나 tunnel의 한 홉만 5 kB/s 대역폭만 사용할 수 있다면, 지연 시간 및 기타 제한 사항과 무관하게 전체 tunnel이 5 kB/s로 제한됩니다.

암호화, 지연 시간, 그리고 터널이 구축되는 방식으로 인해 터널을 구축하는 데 CPU 시간이 상당히 많이 소요됩니다. 이것이 목적지(destination)가 데이터를 전송하기 위해 최대 6개의 인바운드 터널과 6개의 아웃바운드 터널만 가질 수 있는 이유입니다. 터널당 최대 50 kB/s로, 목적지는 대략 300 kB/s의 트래픽을 합산하여 사용할 수 있습니다(실제로는 짧은 터널을 사용하고 익명성이 낮거나 없는 경우 더 많을 수 있습니다). 사용된 터널은 10분마다 폐기되고 새로운 터널이 구축됩니다. 이러한 터널 변경과 때때로 클라이언트가 종료되거나 네트워크 연결이 끊어지면 터널과 연결이 중단될 수 있습니다. 이에 대한 예는 IRC2P 네트워크에서 연결 손실(ping timeout) 또는 eepget 사용 시 확인할 수 있습니다.

제한된 목적지 집합과 목적지당 제한된 터널 집합을 사용하면, 하나의 I2P 노드는 다른 I2P 노드들을 거치는 제한된 터널 집합만을 사용합니다. 예를 들어, I2P 노드가 위의 간단한 예시에서 "hop1"인 경우, 클라이언트로부터 시작되는 하나의 참여 터널만 보게 됩니다. I2P 네트워크 전체를 합산하면, 제한된 양의 대역폭으로 구축할 수 있는 참여 터널의 수는 상당히 제한적입니다. 이러한 제한된 수를 I2P 노드의 수로 분산하면, 사용 가능한 대역폭/용량의 일부만이 사용 가능합니다.

익명성을 유지하기 위해, 하나의 router가 전체 네트워크에서 tunnel을 구축하는 데 사용되어서는 안 됩니다. 만약 하나의 router가 모든 I2P 노드의 tunnel router 역할을 한다면, 이는 매우 현실적인 단일 장애 지점이 될 뿐만 아니라 클라이언트로부터 IP와 데이터를 수집하는 중앙 지점이 됩니다. 이것이 바로 네트워크가 tunnel 구축 프로세스에서 노드 간에 트래픽을 분산시키는 이유입니다.

성능에 대한 또 다른 고려사항은 I2P가 메시 네트워킹을 처리하는 방식입니다. 각 홉 간 연결은 I2P 노드에서 하나의 TCP 또는 UDP 연결을 사용합니다. 1000개의 연결이 있으면 1000개의 TCP 연결이 생성됩니다. 이는 상당히 많은 수이며, 일부 가정용 및 소규모 사무실 라우터는 제한된 수의 연결만 허용합니다. I2P는 이러한 연결을 UDP 및 TCP 유형당 각각 1500개 미만으로 제한하려고 시도합니다. 이는 I2P 노드를 통해 라우팅되는 트래픽의 양도 제한합니다.

노드가 도달 가능하고 대역폭 설정이 128 kB/s 이상 공유되며 24시간 연중무휴로 도달 가능한 경우, 일정 시간 후에 참여 트래픽에 사용되어야 합니다. 중간에 다운되면 다른 노드들이 수행하는 I2P 노드 테스트를 통해 해당 노드가 도달 불가능함을 알게 됩니다. 이렇게 되면 다른 노드들에서 해당 노드가 최소 24시간 동안 차단됩니다. 따라서 해당 노드를 다운 상태로 테스트한 다른 노드들은 24시간 동안 tunnel 구축에 그 노드를 사용하지 않습니다. 이것이 I2P router를 재시작/종료한 후 최소 24시간 동안 트래픽이 낮아지는 이유입니다.

추가로, 다른 I2P 노드들은 특정 I2P router를 테스트하여 도달 가능성과 용량을 확인하기 위해 해당 라우터를 알아야 합니다. 이 프로세스는 네트워크와 상호작용할 때 더 빠르게 진행될 수 있습니다. 예를 들어 애플리케이션을 사용하거나 I2P 사이트를 방문하면 더 많은 tunnel 구축이 이루어지고, 따라서 네트워크상의 노드들이 테스트할 수 있는 활동과 도달 가능성이 증가합니다.

## 성능 기록 (선택됨)

수년에 걸쳐 I2P는 여러 가지 주목할 만한 성능 개선을 이루어왔습니다:

### Native math

GNU MP 라이브러리(GMP)에 대한 JNI 바인딩을 통해 구현되어 이전에 CPU 시간을 지배하던 BigInteger `modPow`를 가속화합니다. 초기 결과는 공개 키 암호화에서 극적인 속도 향상을 보여주었습니다. 참조: /misc/jbigi/

### Garlic wrapping a "reply" LeaseSet (tuned)

이전에는 응답을 위해 발신자의 LeaseSet을 network database에서 조회해야 하는 경우가 많았습니다. 발신자의 LeaseSet을 초기 garlic에 포함시키면 응답 지연 시간이 개선됩니다. 이제는 오버헤드를 줄이기 위해 선택적으로(연결 시작 시점 또는 LeaseSet 변경 시) 수행됩니다.

### 네이티브 수학

잘못된 피어(잘못된 시계, 잘못된 NAT/방화벽, 호환되지 않는 버전)를 더 빨리 거부하기 위해 일부 검증 단계를 transport handshake의 초기 단계로 이동하여 CPU와 대역폭을 절약했습니다.

### Garlic wrapping "reply" LeaseSet (조정됨)

컨텍스트 인식 tunnel 테스트 사용: 이미 데이터를 전달하고 있는 것으로 알려진 tunnel은 테스트하지 않고, 유휴 상태일 때 테스트를 우선합니다. 이를 통해 오버헤드를 줄이고 실패한 tunnel을 더 빠르게 감지할 수 있습니다.

### 더 효율적인 TCP 거부

특정 연결에 대한 선택을 유지하면 순서가 바뀐 전달을 줄이고 스트리밍 라이브러리가 윈도우 크기를 증가시켜 처리량을 향상시킬 수 있습니다.

### 터널 테스트 조정

GZip 또는 유사한 압축 방식을 사용하여 장황한 구조(예: RouterInfo 옵션)를 압축하면 적절한 경우 대역폭을 줄일 수 있습니다.

### 지속적인 터널/lease 선택

단순한 "ministreaming" 프로토콜을 대체합니다. 현대적인 streaming은 I2P의 익명성 기반 메시지 지향 기반에 맞춰진 선택적 ACK와 혼잡 제어를 포함합니다. 참조: /docs/api/streaming/

## Future Performance Improvements (historical ideas)

아래는 잠재적 개선 사항으로 역사적으로 문서화된 아이디어들입니다. 많은 것들이 이미 구식이거나, 구현되었거나, 아키텍처 변경으로 대체되었습니다.

### 선택한 데이터 구조 압축

강력한 적대자에 의한 시빌 공격에 대한 저항성을 유지하면서, 느리거나 과부하된 router를 피하기 위해 tunnel 구축을 위한 피어 선택 방식을 개선합니다.

### 전체 스트리밍 프로토콜

키 공간이 안정적일 때 불필요한 탐색을 줄이고, lookup에서 반환되는 피어 수와 동시에 수행되는 검색 수를 조정합니다.

### Session Tag tuning and improvements (legacy)

레거시 ElGamal/AES+SessionTag 방식의 경우, 더 스마트한 만료 및 보충 전략이 ElGamal 폴백과 낭비되는 태그를 줄입니다.

### 향상된 피어 프로파일링 및 선택

새로운 세션 수립 중 시드된 동기화된 PRNG로부터 태그를 생성하여, 사전 전달된 태그로 인한 메시지당 오버헤드를 줄입니다.

### 네트워크 데이터베이스 튜닝

복구 기능과 결합된 더 긴 터널 수명은 재구축 오버헤드를 줄일 수 있습니다. 익명성 및 안정성과의 균형을 고려하십시오.

### Session Tag 튜닝 및 개선사항 (레거시)

유효하지 않은 피어를 조기에 거부하고 터널 테스트를 보다 컨텍스트 인식형으로 만들어 경합과 지연을 줄입니다.

### SessionTag를 동기화된 PRNG로 마이그레이션 (레거시)

선택적 LeaseSet 번들링, 압축된 RouterInfo 옵션, 그리고 전체 스트리밍 프로토콜의 채택은 모두 체감 성능 향상에 기여합니다.

---


참고:

- [터널 라우팅](/docs/overview/tunnel-routing/)
- [피어 선택](/docs/overview/tunnel-routing/)
- [전송 계층](/docs/overview/transport/)
- [SSU2 명세서](/docs/specs/ssu2/) 및 [NTCP2 명세서](/docs/specs/ntcp2/)
